I"Ù=<h2 id="2020-predicting-customer-class-using-customer-lifetime-value-with-random-forest-algorithm">[2020] Predicting Customer Class using Customer Lifetime Value with Random Forest Algorithm</h2>

<p>Abstract
ë‚´ë…„ ê³ ê° ë“±ê¸‰(CLV)ì„ ì˜ˆì¸¡ â†’ ì˜¨ë¼ì¸ ì†Œë§¤ ì—…ì²´ê°€ ì¥ê¸° CRMì„ ì–»ê¸° ìœ„í•´ ì–´ë–¤ ê³ ê°ì„ íˆ¬ìí•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•˜ëŠ” ë° ë„ì›€ì„ ì¤Œ.</p>
<ul>
  <li>Data: Super Store Retail 4ë…„ ê°„ì˜ ê±°ë˜ ë‚´ì—­ ë°ì´í„°. (ì•½ 1ë§Œ ê±´)</li>
  <li>Random Forest Algorithm í›ˆë ¨ + Random Search tuning ìˆ˜í–‰ â‡’ Best Accuracy</li>
</ul>

<h3 id="1-introduction">1. Introduction</h3>

<p>ì‹ ê·œ ê³ ê°ì„ í™•ë³´í•˜ëŠ” ë° ë“œëŠ” ë¹„ìš©ì´ ê¸°ì¡´ ê³ ê°ì„ ìœ ì§€í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë¹„ì‹¸ê¸° ë•Œë¬¸ì— ì˜¨ë¼ì¸ ì†Œë§¤ ì—…ì²´ëŠ” ê¸°ì¡´ ê³ ê°ì—ê²Œ ì§‘ì¤‘í•´ì•¼ í•œë‹¤. ë˜, ë§ˆì¼€íŒ…, íŒë§¤ ë° ì„œë¹„ìŠ¤ ë¹„ìš©ì´ ìˆ˜ìµì„ ì´ˆê³¼í•  ìˆ˜ ìˆëŠ” ê³ ê°ì´ ìˆì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì˜¨ë¼ì¸ ì†Œë§¤ ì—…ì²´ëŠ” long term CRMì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ê°€ ê°€ì¥ ë†’ì€ ê³ ê°ì—ê²Œ ì§‘ì¤‘í•´ì•¼ í•œë‹¤. ê³ ê°ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ëŠ” ê³ ê°ì´ í‰ìƒ ë™ì•ˆ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì§€ì¶œí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì´ ê¸ˆì•¡ì„ ë‚˜íƒ€ë‚´ëŠ” CLVë¡œ í‘œí˜„ëœë‹¤.</p>

<p>ëŒ€ë¶€ë¶„ì˜ ì—°êµ¬ìë“¤ì€ CLV ë¬¸ì œë¥¼ regression taskìœ¼ë¡œ êµ¬í˜„í–ˆì§€ë§Œ ë³¸ ì—°êµ¬ëŠ” CLVì˜ ë²”ì£¼(ê³ ê° ë“±ê¸‰)ë¥¼ ì˜ˆì¸¡í–ˆë‹¤. ë¶„ë¥˜ ë¶„ì„ì€ ì†Œë§¤ì—…ì²´ê°€ ë§ˆì¼€íŒ… ë¹„ìš©ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµì— íš¨ê³¼ì ìœ¼ë¡œ í• ë‹¹í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ë” ë§ì€ ì •ë³´ë¥¼ ì£¼ê¸°(ìœ ìµí•˜ê¸°) ë•Œë¬¸ì´ë‹¤.</p>

<p>ì˜ˆì¸¡ ëª¨ë¸ì€ RF ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ê³¼ í•¨ê»˜ ì‚¬ìš© ëœ ì†Œë§¤ ê±°ë˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ ë‚´ë…„ ê³ ê°ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œë‹¤. í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ëœ ë°ì´í„° ì„¸íŠ¸ëŠ” ìºê¸€ì˜ 4ë…„ (from 2011 to 2014)ì˜ íŠ¸ëœì­ì…˜ ë°ì´í„°ë¡œ êµ¬ì„±ëœë‹¤.</p>

<p><strong>Process</strong></p>
<ol>
  <li>dataset preprocessing
    <ul>
      <li>cleaning the data</li>
      <li>setting interval (feature and target period for training and testing)
 ê¸°ê°„ ì„¤ì • (í•™ìŠµ ë° ë°ì´íŠ¸ë¥¼ ìœ„í•œ íŠ¹ì§• ë° ëª©í‘œ ê¸°ê°„)</li>
      <li>aggregating(ì§‘ê³„) the data</li>
      <li>discretizing(ì´ì‚°í™”) target variables(CLV) and feature extraction (predictor variables)</li>
    </ul>
  </li>
  <li>
    <p>Merge the datasets of Feature and Target period for training and testing respectively.
í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê°ê° ê¸°ëŠ¥ ë° ëª©í‘œ ê¸°ê°„ì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë³‘í•©í•œë‹¤.</p>
  </li>
  <li>
    <p>Feature selection is made on the training set</p>
  </li>
  <li>
    <p>Random Search cross-validating is conducted to find the optimal hyperparameter values of Random Forest to achieve the best accuracy.
Random Forestì˜ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ì°¾ì•„ ìµœê³ ì˜ ì •í™•ë„ë¥¼ ì–»ê¸° ìœ„í•´ Random Search êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•œë‹¤.</p>
  </li>
  <li>Evaluate modelâ€™s performance on the test dataset.</li>
</ol>

<h3 id="2-related-work">2. Related work</h3>
<p>(Bernat, 2019)ëŠ” ê°œë³„ ê³ ê°ì— ëŒ€í•œ CLVë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ Pareto NBD, Cox ë¹„ë¡€ ìœ„í—˜ ë° Gradient Tree Boostingì˜ ì„¸ ê°€ì§€ ëª¨ë¸ì˜ ì˜ˆì¸¡ë ¥ì„ ë¹„êµí•¨. ì´ ëª¨ë¸ì€ ê³¼ê±° íŠ¸ëœì­ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë‚´ë…„ ê³ ê° ì§€ì¶œì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨. RFM ë³€ìˆ˜ ì™¸ì—ë„ ë‹¤ë¥¸ ê³µë³€ëŸ‰ì„ ì‚¬ìš©í•¨. ê·¸ ì¤‘ Pareto NBD í™•ì¥ ëª¨ë¸ì€ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ.</p>

<p>(Jasek P, 2018)ì€ 6 ê°œì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ í™•ì¥ Pareto NBD ëª¨ë¸, Markov ì²´ì¸ ëª¨ë¸ ë° Status Quo ëª¨ë¸ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•¨. ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì¥ë‹¨ê¸° ëª¨ë‘ì— ëŒ€í•´ í‰ê°€ë¨. EP NBD ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ì˜ í‰ê°€ ì§€í‘œì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í–ˆìŒ.</p>

<p>(Chamberlain, 2017)ì€ RF íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë‚´ë…„ ëª¨ë¸ë§ ê³ ê°ì˜ ìˆœ ì§€ì¶œì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì§€ë‚œ 3 ë…„ê°„ì˜ ë°ì´í„°ì— ëŒ€í•œ í’ë¶€í•œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” ì˜êµ­ ê¸°ë°˜ ê¸€ë¡œë²Œ ì „ì ìƒê±°ë˜ íšŒì‚¬ë¥¼ ìœ„í•œ CLV ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ ê°œë°œí•¨. ë‘ ê°€ì§€ ë¬¸ì œ (CLV ë° ì´íƒˆ ì˜ˆì¸¡)ë¥¼ í•´ê²°í•˜ê³  ê²°ê³¼ë¥¼ í‰ê°€í–ˆìŒ. ê·¸ëŸ° ë‹¤ìŒ ê¸°ëŠ¥ í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ê²°í•©í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ ì‹¤í—˜í•˜ì—¬ ëª¨ë¸ì„ ê°œì„ ì‹œí‚´.</p>

<p>(Nicolas Glady, 2008) ì´íƒˆìë¥¼ CLVê°€ ê°ì†Œí•˜ëŠ” ì‚¬ëŒìœ¼ë¡œ ì •ì˜í–ˆìŒ. ì´ ë…¼ë¬¸ì€ CLV ê°ì†Œë¡œ ì¸í•´ ë°œìƒí•œ ì†ì‹¤ì´ ê³ ê°ì„ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ë¹„ìš©ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë„ì…í–ˆìŒ. ë¡œì§€ìŠ¤í‹± íšŒê·€, ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ì‹ ê²½ë§, ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬, ë¹„ìš©ì— ë¯¼ê°í•œ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ ë° AdaCost ë¶€ìŠ¤íŒ…ì˜ ë‹¤ì„¯ ê°€ì§€ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ ë¹„êµí•¨. AdaCost ë¶„ë¥˜ê¸°ì™€ ë¹„ìš©ì— ë¯¼ê°í•œ íŠ¸ë¦¬ëŠ” ê²½í—˜ì  ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ìµœìƒì˜ ê²°ê³¼ë¥¼ ì–»ì—ˆìŒ. ì‹ ê²½ë§ê³¼ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ëŠ” AUROCì—ì„œ ìµœìƒì˜ ê²°ê³¼ë¥¼ ì œê³µí•¨.</p>

<h3 id="3-model-implementation">3. Model Implementation</h3>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2455f923-8fce-41fa-8a6a-b95584ee2f4e/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2455f923-8fce-41fa-8a6a-b95584ee2f4e/Untitled.png" /></p>

<p>Figure 1 Overview design of the proposed system development</p>

<ul>
  <li>
    <p><strong>3.1. Description and cleaning of the dataset</strong></p>

    <p>The global Superstore dataset from Kaggle</p>
    <ul>
      <li>51,300 order_line rows (25,000 unique Order transactions &amp; 1,500 customers within a purchase period of 4 years (2011 -2014))</li>
    </ul>

    <p>The original dataset contains the 24 attributes: Row ID, Order ID, Order Date, Ship Date, Ship Mode, Customer ID, Customer Name, Segment, City, State, Country, Postal Code, Market, Region, Product ID, Category, Sub- Category, Product Name, Sales, Quantity, Discount, Profit, Shipping Cost, Order Priority.</p>

    <ul>
      <li>In order to get a set of customer behavioral features for our model
  drop the order-related attributes such as Order Priority, Category, Product Name, etc. that cannot improve our model performance.</li>
      <li>
        <p>Categorical attribute has too many unique values â†’ tree-based algorithmsâ€™ predictive power diminished.
  â‡’ Drop high unique values attributes, and are described in Table 1.</p>

        <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc45a6da-8b4c-4bdb-ae0a-e229f02d9d9f/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc45a6da-8b4c-4bdb-ae0a-e229f02d9d9f/Untitled.png" /></p>

        <p>Table 1 Count of unique values of Categoricla attributes</p>
      </li>
    </ul>

    <p>Drop â€˜Countryâ€™, â€˜City, â€˜Stateâ€™ and â€˜Regionâ€™ to get more insight into customersâ€™ purchase behavior.</p>

    <ul>
      <li>Figure 2: the number of orders increasing slightly every year</li>
      <li>
        <p>Figure 3: the number of customers per frequency by customers give more understanding of customer purchase behavior.</p>

        <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/beaa0d83-1ff7-4aa4-876b-3ba44067b476/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/beaa0d83-1ff7-4aa4-876b-3ba44067b476/Untitled.png" /></p>

        <p>Figure 2 Number of orders per year</p>

        <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2093fe6a-de2e-42d7-a5ba-9fd6bd74a17e/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2093fe6a-de2e-42d7-a5ba-9fd6bd74a17e/Untitled.png" /></p>

        <p>Figure 3 Number of customers per frequency</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>3.2. Data pre-processing</strong></p>

    <p>Order date of the dataset is changed into the DateTime format. (ê³„ì‚°í•˜ê¸° ìœ„í•´)</p>

    <p><strong>Step1) Setting Interval</strong></p>

    <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ead432d8-9704-4df1-b005-b5e839228f64/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ead432d8-9704-4df1-b005-b5e839228f64/Untitled.png" /></p>

    <p>Figure 4 Training and testing period of our model</p>

    <p><strong>Step2) Data Aggregation</strong></p>

    <ul>
      <li>For all the four period datasets, we create an â€˜Ordersâ€™ dataframe (grouping order by Order_Id)
        <ul>
          <li>Total_Amount(the sum of Sales)</li>
          <li>Total_Profit(the sum of Profit</li>
          <li>Total_Discount(sum of Discount)</li>
        </ul>
      </li>
      <li>Using the Orders dataset, we create a new dataframe â€˜Customersâ€™ by grouping by Customer_ID to get unique customers.</li>
      <li>Calculate RFM.
        <ul>
          <li>Recency (the day of a customerâ€™s last purchase) - (the last day of the observation period)
  ê³ ê°ì˜ ë§ˆì§€ë§‰ êµ¬ë§¤ ì¼ì—ì„œ ê´€ì°° ê¸°ê°„ì˜ ë§ˆì§€ë§‰ ë‚ ì„ ëº€ ì´ ì¼ìˆ˜</li>
          <li>Frequency (the number of orders a customer made during the observation period)
  ê´€ì°° ê¸°ê°„ ë™ì•ˆ ê³ ê°ì´ ì£¼ë¬¸í•œ ìˆ˜</li>
          <li>Monetary value (the sum of Total_Amount of all orders made by a customer)
  ê³ ê°ì´ ì£¼ë¬¸í•œ ëª¨ë“  ì£¼ë¬¸ì˜ Total_Amount í•©ê³„</li>
        </ul>
      </li>
    </ul>

    <p><strong>Step3) Feature Encoding</strong></p>

    <p>one-hot encoding (categorical â†’ numerical format)</p>

    <p><strong>Step3) Discretizing Target variable</strong></p>

    <p>The dependent variable â€˜CLVâ€™ is calculated based on the Customers dataset using the following equations (1) to (7).</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. CLV = (ê³ ê° ê°€ì¹˜ / ì´íƒˆë¥ ) * ì´ìµ ë§ˆì§„
  2. ê³ ê° ê°€ì¹˜ = í‰ê·  ì£¼ë¬¸ ê°€ì¹˜ * êµ¬ë§¤ ë¹ˆë„
  3. ì´íƒˆë¥  = 1 - ë°˜ë³µë¥ 
  4. ì´ìµ ë§ˆì§„ = ì´ ìˆ˜ìµ * ì´ìœ¤ìœ¨(%)
  5. í‰ê·  ì£¼ë¬¸ ê°’ = ì´ ìˆ˜ìµ / ê°œë³„ ê³ ê°ì˜ ê±°ë˜ ìˆ˜
  6. êµ¬ë§¤ ë¹ˆë„ = ì „ì²´ ê³ ê°ì˜ ì´ ê±°ë˜ ìˆ˜ / ê³ ê° ìˆ˜
  7. ë°˜ë³µ ë¹„ìœ¨ = ê±°ë¦¬ íšŸìˆ˜ê°€ 1ë³´ë‹¤ í° ê³ ê° ë¹„ìœ¨
</code></pre></div>    </div>

    <p>Customer Class</p>

    <ul>
      <li>0: High-Class customer / 1: Low-Class customer</li>
    </ul>

    <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d43e4242-5438-4ac3-8f42-754df59c4f3b/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d43e4242-5438-4ac3-8f42-754df59c4f3b/Untitled.png" /></p>

    <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b6095b55-110b-4d63-b582-79624fa123ac/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b6095b55-110b-4d63-b582-79624fa123ac/Untitled.png" /></p>

    <p>Table 2 Three rows with discretization of target variable</p>

    <p><strong>Step4) Merge Feature and Target variable</strong></p>

    <p><strong>Step5) Feature Selection</strong></p>

    <p>Apply feature importance to rank the most important variables from our features set using the Random Forest feature importance method.</p>

    <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/da19180c-e762-4685-9348-b97db19fa948/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/da19180c-e762-4685-9348-b97db19fa948/Untitled.png" /></p>

    <p>Table 3 Feature importance of selected features</p>
  </li>
  <li>
    <p><strong>3.3. Random Forest</strong></p>

    <p>Random forest: a supervised machine learning algorithm that can be used for both regression and classification problems.</p>

    <p>In the Random Forest algorithm, there are two stages:</p>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Random Forest creation pseudocode:
â†’ kê°œì˜ featureë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ
â†’ kê°œì˜ feature ì¤‘ the best split pointì„ ì´ìš©í•˜ì—¬ ë…¸ë“œ dë¥¼ ê³„ì‚°
â†’ the best split pointë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ìœ„ ë…¸ë“œë¡œ ë¶„í• 
â†’ ë…¸ë“œì˜ ê°œìˆ˜ê°€ Iê°œê°€ ë  ë•Œê¹Œì§€ ìœ„ ì„¸ë‹¨ê³„ë¥¼ ë°˜ë³µ
â†’ nê°œì˜ íŠ¸ë¦¬ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ìœ„ ë‹¨ê³„ë¥¼ në²ˆ ë°˜ë³µ
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```
**Random forest prediction pseudocode**:
â†’ ë¬´ì‘ìœ„ë¡œ ìƒì„±ëœ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ì˜ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê³  ì €ì¥
â†’ ì˜ˆì¸¡ëœ targetì— íˆ¬í‘œ
â†’ íˆ¬í‘œìœ¨ì´ ë†’ì€ ì˜ˆì¸¡ target = ìµœì¢… ì˜ˆì¸¡ target
```
</code></pre></div></div>

<ul>
  <li><strong>3.4. Hyperparameter tuning</strong>
    <ul>
      <li>ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜: ì…ë ¥ ë°ì´í„°ë¥¼ ì›í•˜ëŠ” ì¶œë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ì§€ì •í•˜ëŠ” í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ ì¤‘ì— ì¶”ì •ëœ ê°’.</li>
      <li>í•˜ì´í¼ íŒŒë¼ë¯¸í„°: ëª¨ë¸ ì •í™•ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•¨.</li>
    </ul>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°
  - max_samples: ê° ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ë¥¼ í›ˆë ¨í•˜ëŠ” ìƒ˜í”Œ ìˆ˜
  - max_features: ìµœì  ë¶„í• ì„ ì°¾ì„ ë•Œ ê³ ë ¤í•  íŠ¹ì„± ìˆ˜
  - n_estimators: í¬ë¦¬ìŠ¤íŠ¸ì˜ íŠ¸ë¦¬ ìˆ˜
  - criterion: ë¶„í•  í’ˆì§ˆì„ ì¸¡ì •í•˜ëŠ” ê¸°ëŠ¥
  - max_depth: íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
  - min_samples_leaf: ë¦¬í”„ ë…¸ë“œì— ìˆì–´ì•¼í•˜ëŠ” ìµœì†Œ ìƒ˜í”Œ ìˆ˜
  - min_samples_split: ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
</code></pre></div>    </div>

    <p>í–¥ìƒëœ íƒìƒ‰ ëŠ¥ë ¥, ì„ê³„ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ìµœì ê°’ì„ ì°¾ê³  í›¨ì”¬ ë” ì§§ì€ ì‹œê°„ì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— Random Search ì‚¬ìš©. (Random Search: í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë¬´ì‘ìœ„ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì— ê°€ì¥ ì í•©í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì°¾ëŠ” ê¸°ìˆ )</p>

    <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1d8f778d-11c4-4d39-9f5a-ffc77ebab236/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1d8f778d-11c4-4d39-9f5a-ffc77ebab236/Untitled.png" /></p>

    <p>Table 4 Initialized values and optimal hyperparameters values of random search of model</p>
  </li>
</ul>

<h3 id="4-performance-measure-and-results">4. Performance measure and Results</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>**classification_report**
- ì •ë°€ë„(Precision): í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ ìƒ˜í”Œ ì¤‘ ì‹¤ì œë¡œ ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨
- ì¬í˜„ìœ¨(Recall): ì‹¤ì œ ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•œ í‘œë³¸ ì¤‘ì— ì–‘ì„± í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ í‘œë³¸ì˜ ìˆ˜ì˜ ë¹„ìœ¨
- f1-score: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ê°€ì¤‘ì¡°í™”í‰ê· (weight harmonic average)
- Accuracy: ì „ì²´ ìƒ˜í”Œ ì¤‘ ë§ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨
</code></pre></div></div>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/75f182fa-6e68-442f-a21b-21ada025454b/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/75f182fa-6e68-442f-a21b-21ada025454b/Untitled.png" /></p>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a0d2ef91-3816-4307-90b9-d50a4bace44a/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a0d2ef91-3816-4307-90b9-d50a4bace44a/Untitled.png" /></p>

<p>Table 5 Accuracy of classifier models with different hyperparametersâ€™ sets on testing dataset</p>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/98a7b4b5-b143-4051-8b69-4306e60005a9/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/98a7b4b5-b143-4051-8b69-4306e60005a9/Untitled.png" /></p>

<p>Table 6 Resulted precision, recall, and f1-score for each customer class of best model</p>
:ET